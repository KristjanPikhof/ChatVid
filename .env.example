# ChatVid Configuration
# Copy this file to .env and configure your settings

# ============================================================================
# API Configuration (Required)
# ============================================================================

# For OpenAI (direct):
# Uncomment the line below to switch to OpenRouter or other custom endpoint:
# OPENAI_API_BASE=https://openrouter.ai/api/v1
OPENAI_API_KEY=sk-your-openai-key-here

# For OpenRouter:
# 1. Uncomment OPENAI_API_BASE line above
# 2. Replace API key with your OpenRouter key (sk-or-v1-...)
# 3. Update LLM_MODEL below to use provider/model format (e.g., openai/gpt-4o)

# ============================================================================
# Chunking Configuration (Build Phase)
# ============================================================================

# Chunking strategy - Phase 2 Feature (v1.6.0)
# Options: "semantic" or "fixed"
# Default: semantic (recommended for better quality)
# - semantic: Respects sentence boundaries, preserves context (+25% quality)
# - fixed: Simple character-based splits (faster, backward compatible)
CHUNKING_STRATEGY=semantic

# Target chunk size in characters (used as middle point for semantic chunking)
# Range: 100-1000 | Default: 500
# Smaller = more granular, better for short queries
# Larger = more context per chunk, better for complex questions
CHUNK_SIZE=500

# Overlap for fixed chunking (character count)
# Range: 20-200 | Default: 50
# Only used when CHUNKING_STRATEGY=fixed
CHUNK_OVERLAP=50

# Semantic chunking configuration (Phase 2 - v1.6.0)
# These settings only apply when CHUNKING_STRATEGY=semantic

# Minimum chunk size (prevents tiny chunks)
# Range: 100-500 | Default: 300
MIN_CHUNK_SIZE=300

# Maximum chunk size (prevents oversized chunks)
# Range: 500-2000 | Default: 700
MAX_CHUNK_SIZE=700

# Number of sentences to overlap between chunks
# Range: 0-3 | Default: 1
# Ensures continuity and prevents information loss at boundaries
OVERLAP_SENTENCES=1

# Sentence detection backend for semantic chunking
# Options: "regex" (fastest, default), "nltk" (better accuracy), "spacy" (best accuracy)
# Default: regex (no dependencies, 10-100x faster than NLTK)
# - regex: Pattern-based, no dependencies, fast (~0.0s for 80K chars)
# - nltk: Natural language toolkit, auto-downloads punkt data (~0.5s for 80K chars)
# - spacy: Most accurate, requires manual model download (python -m spacy download en_core_web_sm)
SENTENCE_BACKEND=nltk

# ============================================================================
# Document Processing Configuration (Build Phase)
# ============================================================================

# Maximum rows to extract per sheet when processing spreadsheets
# Range: 1000-50000 | Default: 10000
# Prevents memory issues with very large Excel/CSV files
# Set to higher value if you have large datasets and sufficient RAM
MAX_SPREADSHEET_ROWS=10000

# Enable metadata enrichment (adds document context to chunks)
# Default: true | Phase 1 Feature (v1.5.0)
# Improves LLM understanding by 15-20% with document metadata prefixes
# Example: [Document: report.pdf | Pages: 10 | Author: Name]
ENABLE_METADATA_ENRICHMENT=true

# ============================================================================
# Adaptive Retrieval Configuration (Chat Phase) - Phase 1 Feature (v1.5.0)
# ============================================================================

# Enable adaptive retrieval (automatically adjusts chunk count based on query)
# Default: true | Recommended for better broad question handling
# When enabled, broad questions retrieve more chunks (up to MAX_TOP_K)
# When disabled, uses fixed CONTEXT_CHUNKS value
ENABLE_ADAPTIVE_TOP_K=true

# Minimum chunks to retrieve for specific questions
# Range: 1-20 | Default: 5
# Used for simple, specific queries like "What is X?"
MIN_TOP_K=5

# Maximum chunks to retrieve for broad questions
# Range: 5-50 | Default: 25
# Used for complex, exploratory queries like "Explain Y comprehensively"
# Note: Higher values increase token usage and API costs
MAX_TOP_K=25

# Debug mode: Show query analysis and chunk count decisions
# Default: false | Useful for understanding adaptive behavior
DEBUG_ADAPTIVE=false

# ============================================================================
# LLM Configuration (Chat Phase)
# ============================================================================

# Model to use for chat responses
# OpenAI: gpt-4o-mini-2024-07-18, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# OpenRouter: openai/gpt-4o, anthropic/claude-3-sonnet, google/gemini-pro-1.5, etc.
# Default: gpt-4o-mini-2024-07-18 (OpenAI) or openai/gpt-4o (OpenRouter)
LLM_MODEL=gpt-4o-mini-2024-07-18

# Temperature controls response creativity (0.0 = focused, 2.0 = creative)
# Range: 0.0-2.0 | Default: 0.7
# Lower = more deterministic, Higher = more creative
LLM_TEMPERATURE=0.7

# Maximum tokens in response (controls response length)
# Range: 100-4000 | Default: 1000
# Higher = longer responses but higher API costs
LLM_MAX_TOKENS=1000

# Number of text chunks to retrieve for each query
# Range: 1-20 | Default: 10
# More chunks = better context but higher token usage
CONTEXT_CHUNKS=10

# Number of conversation turns to remember (history depth)
# Range: 1-50 | Default: 10
# Higher = longer memory but higher token usage
MAX_HISTORY=10

# ============================================================================
# Citation & Source Tracking Configuration (Chat Phase) - v1.7.0
# ============================================================================

# Enable citation display in chat responses
# Default: true | Shows which documents answers came from
# When enabled, responses include numbered source references [1][2]
SHOW_CITATIONS=true

# Show hints about /source command after each response
# Default: true | Reminds users they can view full source context
# Example: "ðŸ’¡ Use '/source N' to see full context for source [N]"
SHOW_SOURCE_HINTS=true

# Enable interactive source commands (/source, /sources)
# Default: true | Allows viewing full chunk context during chat
# Commands: /source N (show details), /sources (list all)
ENABLE_SOURCE_COMMANDS=true

# Display full chunk text in /source command
# Default: true | Shows complete chunk with all context
# When false, shows only metadata and short excerpt
SOURCE_DISPLAY_FULL_CHUNK=true

# Show relevance scores in citation display
# Default: false | Advanced feature for debugging retrieval quality
# When enabled, shows similarity scores like "[1] report.pdf (score: 0.89)"
SHOW_RELEVANCE_SCORES=false

# ============================================================================
# Tips & Recommendations
# ============================================================================
#
# For Technical Documents:
#   CHUNK_SIZE=400, CHUNK_OVERLAP=80, LLM_TEMPERATURE=0.3
#
# For Creative Content:
#   CHUNK_SIZE=300, CHUNK_OVERLAP=50, LLM_TEMPERATURE=1.0
#
# For Cost Optimization:
#   LLM_MODEL=gpt-4o-mini-2024-07-18, CONTEXT_CHUNKS=7, LLM_MAX_TOKENS=500
#
# For Maximum Quality:
#   LLM_MODEL=gpt-4o, CONTEXT_CHUNKS=15, LLM_MAX_TOKENS=2000
#
# ============================================================================
