# ChatVid Configuration
# Copy this file to .env and configure your settings

# ============================================================================
# API Configuration (Required)
# ============================================================================

# For OpenAI:
OPENAI_API_KEY=sk-your-openai-key-here

# For OpenRouter (uncomment both lines and comment OpenAI key above):
# OPENAI_API_BASE=https://openrouter.ai/api/v1
# OPENAI_API_KEY=sk-or-v1-your-openrouter-key-here

# ============================================================================
# Chunking Configuration (Build Phase)
# ============================================================================

# Size of text chunks in characters (affects how documents are split)
# Range: 100-1000 | Default: 300
# Smaller = more granular, better for short queries
# Larger = more context per chunk, better for complex questions
CHUNK_SIZE=300

# Overlap between consecutive chunks in characters (prevents information loss)
# Range: 20-200 | Default: 50
# Higher overlap = better continuity but more chunks
CHUNK_OVERLAP=50

# ============================================================================
# LLM Configuration (Chat Phase)
# ============================================================================

# Model to use for chat responses
# OpenAI: gpt-4o-mini-2024-07-18, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# OpenRouter: openai/gpt-4o, anthropic/claude-3-sonnet, google/gemini-pro-1.5, etc.
# Default: gpt-4o-mini-2024-07-18 (OpenAI) or openai/gpt-4o (OpenRouter)
LLM_MODEL=gpt-4o-mini-2024-07-18

# Temperature controls response creativity (0.0 = focused, 2.0 = creative)
# Range: 0.0-2.0 | Default: 0.7
# Lower = more deterministic, Higher = more creative
LLM_TEMPERATURE=0.7

# Maximum tokens in response (controls response length)
# Range: 100-4000 | Default: 1000
# Higher = longer responses but higher API costs
LLM_MAX_TOKENS=1000

# Number of text chunks to retrieve for each query
# Range: 1-20 | Default: 10
# More chunks = better context but higher token usage
CONTEXT_CHUNKS=10

# Number of conversation turns to remember (history depth)
# Range: 1-50 | Default: 10
# Higher = longer memory but higher token usage
MAX_HISTORY=10

# ============================================================================
# Tips & Recommendations
# ============================================================================
#
# For Technical Documents:
#   CHUNK_SIZE=400, CHUNK_OVERLAP=80, LLM_TEMPERATURE=0.3
#
# For Creative Content:
#   CHUNK_SIZE=300, CHUNK_OVERLAP=50, LLM_TEMPERATURE=1.0
#
# For Cost Optimization:
#   LLM_MODEL=gpt-4o-mini-2024-07-18, CONTEXT_CHUNKS=7, LLM_MAX_TOKENS=500
#
# For Maximum Quality:
#   LLM_MODEL=gpt-4o, CONTEXT_CHUNKS=15, LLM_MAX_TOKENS=2000
#
# ============================================================================
